{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are threads?\n",
    "Threads are **execution units within a process** that can run simultaneously. While processes are separate, threads run in a **shared memory** space (heap).\n",
    "\n",
    "<!-- <img src=\"./imgs/what-are-threads.png\" width=500px> -->\n",
    "\n",
    "<br>\n",
    "<img src=\"imgs/stack_heap_threads.svg\" width=450px>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Julia with multiple threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Julia starts with a single *user thread*. We must tell it explicitly to start multiple user threads. There are a couple of ways to do this:\n",
    "\n",
    "* Environment variable: `export JULIA_NUM_THREADS=4`\n",
    "* Command line argument: `julia --threads 4` or `julia -t 4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is currently not (easily) possible to change the number of threads at runtime!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Jupyter, we create another kernel that starts Julia with multiple threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using IJulia\n",
    "installkernel(\"Julia (4 threads)\", \"--project=@.\", env=Dict(\"JULIA_NUM_THREADS\"=>\"4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we need to **refresh the page** and select the new `Julia (4 threads) 1.10` kernel in the top right corner. (Restart Jupyter if the kernel doesn't show up.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can readily check how many threads we are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User threads vs default threads\n",
    "\n",
    "Technically, the Julia process is also spawning multiple threads already in \"single-threaded\" mode, like\n",
    "* a thread for unix signal listening\n",
    "* multiple OpenBLAS threads for BLAS/LAPACK operations\n",
    "* GC threads\n",
    "\n",
    "We call the threads that we can actually run computations on *user threads* or *Julia threads*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "BLAS.get_num_threads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are my threads running?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ThreadPinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threadinfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-based multithreading\n",
    "\n",
    "Julia implements **task-based** multithreading. In this paradigm, a task - e.g. a computational piece of a code - is marked for **parallel** execution on **any** of the Julia threads. Julia's **dynamic scheduler** will put the task on a thread and trigger the execution of the task.\n",
    "\n",
    "<br>\n",
    "<!-- <img src=\"imgs/task-based-parallelism.png\" width=768px> -->\n",
    "<img src=\"imgs/tasks_threads_cores.svg\" width=650px>\n",
    "</br>\n",
    "\n",
    "Task-based multithreading: **The user should think about tasks and not threads**.\n",
    "* By default, the user does not control on which thread a task will run (the task might even [migrate](https://docs.julialang.org/en/v1/manual/multi-threading/#man-task-migration) between threads!).\n",
    "\n",
    "**Advantages:**\n",
    "* high-level abstraction: one can spawn many tasks (>> number of threads)\n",
    "* nestable multithreading\n",
    "\n",
    "**Disadvantages:**\n",
    "* dynamic scheduling overhead\n",
    "* uncertainty and potentially suboptimal task → thread assignment\n",
    "  * can get in the way when performance engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spawning tasks\n",
    "`Threads.@spawn` spawns a task to be run on any Julia thread. Specifically, it creates a `Task` and schedules it for execution on an available Julia thread (we don't control which one!).\n",
    "\n",
    "Note that `Threads.@spawn` is **asynchronous** and **non-blocking**, that is, it doesn't wait for the task to actually run but immediately returns a `Task`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Base.Threads # afterwards we can just write @spawn instead of Threads.@spawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@spawn 3+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fetch the result of a task with `fetch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = @spawn 3+3\n",
    "fetch(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `@spawn` returns right away, `fetch` is **blocking** as it has to wait for the task to actually finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@time t = @spawn begin\n",
    "    sleep(3)\n",
    "    return 3+3\n",
    "end\n",
    "@time fetch(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the macro `@sync` to synchronize all (lexically) encompassed asynchronous operations (`@spawn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@time @sync t = @spawn begin\n",
    "    sleep(3)\n",
    "    return 3+3\n",
    "end\n",
    "@time fetch(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: multithreaded `map`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "`tmap`: *threaded map*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function tmap(f, collection)\n",
    "    tasks = map(x -> @spawn(f(x)), collection)  # for each x ∈ collection, spawn a task to compute f(x)\n",
    "    return fetch.(tasks)                        # fetch and return all the results\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "M = [rand(200,200) for i in 1:8];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmap(svdvals, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@btime tmap($svdvals, $M) samples=10 evals=3;\n",
    "@btime map($svdvals, $M) samples=10 evals=3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use multithreading in Julia in combination with BLAS/LAPACK functions, it is important to carefully consider and configure the [interplay between Julia threads and BLAS threads](https://carstenbauer.github.io/ThreadPinning.jl/stable/explanations/blas/).\n",
    "\n",
    "Easiest way out: turn of BLAS/LAPACK multithreading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BLAS.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@btime tmap($svdvals, $M) samples=10 evals=3;\n",
    "@btime map($svdvals, $M) samples=10 evals=3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: multithreading for-loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ThreadPinning.Utility: taskid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@sync for i in 1:8\n",
    "    @spawn println(\"Task \", taskid(), \" is running iteration \", i, \" on thread \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `@threads`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Splits up the iteration space into `nthreads()` contiguous chunks**\n",
    "* Creates a task for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@threads for i in 1:8\n",
    "    println(\"Task \", taskid(), \" is running iteration \", i, \" on thread \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load-balancing and chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are many tasks (e.g. many more than available threads), Julia's scheduler balances the load of these tasks among threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using StatsPlots\n",
    "using ChunkSplitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package [ChunkSplitters.jl](https://github.com/m3g/ChunkSplitters.jl) is helpful for chunking (`Iterators.partition` is a built-in alternative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is purely pedagogical\n",
    "function tmap_tracking(f, collection; tracker = [UnitRange[] for _ in 1:nthreads()], ntasks=nthreads())\n",
    "    result = zeros(length(collection))\n",
    "    @sync for idcs in chunks(collection; n=ntasks)    # chunk up collection into ntasks-many chunks\n",
    "        Threads.@spawn begin                          # spawn a task for each chunk\n",
    "            for i in idcs                             # for each element of a that belongs to this chunk/task\n",
    "                result[i] = f(collection[i])          # apply f\n",
    "            end\n",
    "            push!(tracker[threadid()], idcs)          # keep track of which thread ran the task\n",
    "        end\n",
    "    end\n",
    "    return result, tracker\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xs = 1:2^7\n",
    "f(x) = sum(abs2, rand() for _ in 1:(2^14*x)) # computational cost is increasing as a function of x (non-uniform)\n",
    "\n",
    "result, tracker = tmap_tracking(f, xs; ntasks=length(xs))   # create a task for each element of `a`\n",
    "# result, tracker = tmap_tracking(f, xs; ntasks=8*nthreads()) # create 8*nthreads() tasks, each handling a chunk of `a`\n",
    "# result, tracker = tmap_tracking(f, xs; ntasks=4*nthreads()) # create 4*nthreads() tasks, each handling a chunk of `a`\n",
    "# result, tracker = tmap_tracking(f, xs; ntasks=2*nthreads()) # create 2*nthreads() tasks, each handling a chunk of `a`\n",
    "# result, tracker = tmap_tracking(f, xs; ntasks=1)            # create a single task, handling all of `a`\n",
    "# result, tracker = tmap_tracking(f, xs; ntasks=nthreads())   # create nthreads() tasks, each handling a chunk of `a`\n",
    "\n",
    "# plotting\n",
    "thread_workloads = zeros(Int, nthreads(), maximum(length, tracker))\n",
    "for th in eachindex(tracker)\n",
    "    for (i, ws) in enumerate(tracker[th])\n",
    "        thread_workloads[th, i] = sum(ws)\n",
    "    end\n",
    "end\n",
    "b = groupedbar(thread_workloads, xlab=\"threadid\", ylab=\"workload\", title=\"@spawn\", legend=false, bar_position=:stack)\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Since `@threads` divides the iteration range into `nthreads()` chunks there is no load balancing: Each thread gets a single task and that's it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Nestable multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Recursive Fibonacci series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ F(n) = F(n-1) + F(n-2), \\qquad F(1) = F(2) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: Algorithmically, this is a highly inefficient implementation of the Fibonacci series!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function fib(n)\n",
    "    n < 2 && return n\n",
    "    t = @spawn fib(n-2)\n",
    "    return fib(n-1) + fetch(t)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are nesting `@spawn` calls recursively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fib(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmap(fib, 1:20) # multithreaded tmap applying a multithreaded fib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Opting out of dynamic scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "For \"traditional HPC\", where you tell each thread what to do, you might want to opt out of dynamic scheduling and task migration. \n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* guaranteed task-thread mapping (\"task pinning\")\n",
    "* lower overhead\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* often less portable code (e.g. hardcoded assumptions about the system)\n",
    "* no (or at least bad) nestability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `@spawnat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia doesn't have a built-in macro for spawning **sticky** tasks on specific threads but many packages, including [ThreadPinning.jl](https://github.com/carstenbauer/ThreadPinning.jl) and [OhMyThreads.jl](https://github.com/JuliaFolds2/OhMyThreads.jl), provide `@spawnat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ThreadPinning: @spawnat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@spawnat 4 println(\"Task \", taskid(), \" is running on thread \", threadid(), \", and always will be 😉\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `@threads :static`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `@threads` there is the `:static` scheduling option to opt-out of Julia's dynamic scheduling.\n",
    "\n",
    "* **statically** maps tasks/chunks to threads, specifically: task 1 → thread 1, task 2 → thread 2, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@threads :dynamic for i in 1:2*nthreads() # :dynamic is the default\n",
    "    println(\"Task \", taskid(), \" is running iteration \", i, \" on thread \", threadid());\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@threads :static for i in 1:2*nthreads()\n",
    "    println(\"Task \", taskid(), \" is running iteration \", i, \" on thread \", threadid());\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `@threads :static`, every thread handles precisely two iterations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Beware of Multithreading: Parallel Summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = rand(1_000_000 * Threads.nthreads());\n",
    "\n",
    "sum(data) # we want to parallelize this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How you should parallelize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The honest answer is: Simply use a package like, e.g., [OhMyThreads.jl](https://github.com/JuliaFolds2/OhMyThreads.jl). There is no need to roll your own parallel summation (or your own `tmap` 😉)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OhMyThreads: treduce\n",
    "\n",
    "treduce(+, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treduce(+, data) ≈ sum(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum($data);\n",
    "@btime treduce($+, $data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's assume we want to write a parallel version ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-focused parallel version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key questions for task-based parallelisation:\n",
    "* How to divide the computation into seperate **tasks**?\n",
    "    * Answer: chunk up the data and perform partial sums.\n",
    "* How many **tasks** should we create?\n",
    "    * Answer: since the workload is uniform, `nthreads()` many tasks is a reasonable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sum_loop_spawn(data; ntasks=nthreads())\n",
    "    ts = Vector{Task}(undef, ntasks)\n",
    "    for (c, idcs) in enumerate(chunks(data; n=ntasks))\n",
    "        ts[c] = @spawn @views sum(data[idcs])\n",
    "    end\n",
    "    return sum(fetch.(ts))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or a bit nicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function sum_map_spawn(data; ntasks=nthreads())\n",
    "    ts = map(chunks(data, n=ntasks)) do idcs\n",
    "        @spawn @views sum(data[idcs])\n",
    "    end\n",
    "    return sum(fetch.(ts))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conceptually simple and task-focused\n",
    "  * → We're **explicitly** spawning one task per chunk.\n",
    "  * → No mention of threads, except in `ntasks=nthreads()`.\n",
    "* In the latter form, we don't even need a manual pre-allocation (it is hidden in the map operation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum_map_spawn(data) ≈ sum(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@btime sum_loop_spawn($data);\n",
    "@btime sum_map_spawn($data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 1: Race condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function sum_threads_naive(data)\n",
    "    s = zero(eltype(data))\n",
    "    @threads for x in eachindex(data)\n",
    "        s += x\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@show sum(data);\n",
    "@show sum_threads_naive(data);\n",
    "@show sum_threads_naive(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrong** result! Even worse, it's **non-deterministic** and different every time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a [race condition](https://en.wikipedia.org/wiki/Race_condition) which typically appear when multiple tasks are modifying shared state simultaneously.\n",
    "\n",
    "→ If possible, **don't modify shared (i.e. non task-local) state!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Thread-focused rather than task-focused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be inclined to write something similar to the following (intentionally written in a slightly more verbose form):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function sum_threads_unsafe(data)\n",
    "    psums = zeros(eltype(data), nthreads())\n",
    "    @threads for i in eachindex(data)\n",
    "        current_sum = psums[threadid()] # read\n",
    "        new_sum = current_sum + data[i] # \"work\"\n",
    "        psums[threadid()] = new_sum     # write\n",
    "    end\n",
    "    return sum(psums)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such an approach is generally **unsafe** because Julia's scheduler may **migrate tasks between threads**!\n",
    "  * For example, a task might start on thread 1, is then paused (say, after \"work\") and migrated to thread 3, where it finishes execution.\n",
    "  * → The output of `threadid()` might change within a task! To be safe, [don't use `threadid()`](https://julialang.org/blog/2023/07/PSA-dont-use-threadid/) at all!\n",
    "  \n",
    "It also goes against the idea of task-based multithreading, as we're **thinking about threads rather than tasks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that, in spite of the comments above, the `threadid()` pattern will often still work correctly. This is because as of Julia 1.10 task migrations are very rare. **You can't rely on it though!**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Performance) Mistake 3: False sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function sum_threads_chunks(data; nchunks=nthreads())\n",
    "    psums = zeros(eltype(data), nchunks)\n",
    "    @threads for (c, idcs) in enumerate(chunks(data; n=nchunks))\n",
    "        for i in idcs\n",
    "            psums[c] += data[i]\n",
    "        end\n",
    "    end\n",
    "    return sum(psums)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum_threads_chunks(data) ≈ sum(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@btime sum($data);\n",
    "@btime sum_threads_chunks($data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safe, but (horribly) slow?! Why?\n",
    "\n",
    "* **False sharing** of `psums`\n",
    "* Also, but less important here: manual loop doesn't SIMD (need to add manual `@simd` + `@inbounds`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance issue: [False sharing](https://en.wikipedia.org/wiki/False_sharing)\n",
    "\n",
    "Why does `sum_threads_chunks` above have bad performance? Although argubaly subtle, this is because different tasks mutate shared data (`psums`) in parallel. There is no *logical* sharing: Tasks access different slots of `psums` and there is no data race. However, CPU cores work on the basis of **cache lines** instead of single elements leading to *implicit* sharing of cache lines.\n",
    "\n",
    "**Despite its subtlety, false sharing can lead to dramatic slowdown!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CpuId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cachelinesize() ÷ sizeof(Float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/false_sharing.svg\" width=850px>\n",
    "\n",
    "Different tasks modify the same cache line\n",
    "* need for synchronization to ensure cache coherency\n",
    "* performance decreases (dramatically).\n",
    "\n",
    "Once agin: **The less you modify shared (i.e. non task-local) state, the better!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \"Fixed\" version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function sum_threads_chunks_local(data; nchunks=nthreads())\n",
    "    psums = zeros(eltype(data), nchunks)\n",
    "    @threads for (c, idcs) in enumerate(chunks(data; n=nchunks))\n",
    "        local s = zero(eltype(data))\n",
    "        @simd for i in idcs\n",
    "            @inbounds s += data[i]\n",
    "        end\n",
    "        psums[c] = s\n",
    "    end\n",
    "    return sum(psums)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* each task/iteration computes a local sum (`s`) independently\n",
    "* no *frequent* non-local mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum(data) ≈ sum_threads_chunks_local(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@btime sum($data);\n",
    "@btime sum_threads_chunks_local($data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Additional comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tools for multi-threading\n",
    "\n",
    "* [OhMyThreads.jl](https://github.com/JuliaFolds2/OhMyThreads.jl): Simple tools for basic multithreading.\n",
    "* [ThreadsX.jl](https://github.com/JuliaFolds2/ThreadsX.jl): Parallelized Base functions\n",
    "* [Tullio.jl](https://github.com/mcabbott/Tullio.jl): Tullio is a very flexible einsum macro ([Einstein notation](https://en.wikipedia.org/wiki/Einstein_notation))\n",
    "* [(LoopVectorization.jl)](https://github.com/JuliaSIMD/LoopVectorization.jl): Macro(s) for vectorizing loops.\n",
    "* [(FLoops.jl)](https://github.com/JuliaFolds/FLoops.jl): Fast sequential, threaded, and distributed for-loops for Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinning Julia threads to CPU threads/cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A compute node has a complex topology (two sockets, multiple memory channels/domains). Placing the Julia threads systematically on CPU-threads matters for\n",
    "\n",
    "* the computation performance of your Julia codes\n",
    "* fluctuations/noises in benchmarks\n",
    "* hardware-level performance monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ThreadPinning.jl\n",
    "\n",
    "<!-- <br>\n",
    "<img src=\"imgs/threadpinning_pinthreads.svg\" width=600px>\n",
    "</br> -->\n",
    "\n",
    "`pinthreads(strategy)`\n",
    "* `:cputhreads` pin to CPU threads (incl. \"hypterthreads\") one after another\n",
    "* `:cores:` pin to CPU cores one after another\n",
    "* `:numa:` round-robin between NUMA domains\n",
    "* `:sockets:` round-robin between sockets\n",
    "* `:affinitymask`: according to an external affinity mask (e.g. set by SLURM)\n",
    "\n",
    "(More? See my short talk at JuliaCon2023 @ MIT: https://youtu.be/6Whc9XtlCC0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threadinfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinthreads(:cores)\n",
    "threadinfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll explore the effect of thread pinning on performance in more detail later → **daxpy_cpu exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garbage collection\n",
    "\n",
    "If it gets triggered, it stops the world (all threads) for clearing up memory.\n",
    "\n",
    "Hence, when using multithreading, it is even more important to **avoid heap allocations!**\n",
    "\n",
    "(If you can't avoid allocations, consider using multiprocessing instead.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atomic operations and locks\n",
    "\n",
    "See [Atomic Operations](https://docs.julialang.org/en/v1/manual/multi-threading/#Atomic-Operations) and/or [Data-race freedom](https://docs.julialang.org/en/v1/manual/multi-threading/#Data-race-freedom) in the Julia doc for more information. In general, one should avoid using them as much as possible since they actually limit the parallelization by serialized executions (especially if you don't know what you're doing). That said, locks can be an effective way to use a data structures that themselves aren't thread safe, e.g. `Dict`."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia (4 threads) 1.10.4",
   "language": "julia",
   "name": "julia-_4-threads_-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
